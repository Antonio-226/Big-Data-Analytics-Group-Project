{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.context import SparkContext\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import optimizers, regularizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up spark context and increase available memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '6g'),\n",
       " ('spark.driver.port', '54406'),\n",
       " ('spark.driver.host', 'DESKTOP-RE98QFK'),\n",
       " ('spark.sql.crossJoin.enabled', 'true'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.id', 'local-1622810173134'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'Spark DL Tabular Pipeline')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('Spark DL Tabular Pipeline').setMaster('local[*]')\n",
    "conf = conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "# Increase memory spark has available\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '6g')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "#pyspark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "\n",
    "#Verify settings set for available memory\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- air_temperature: double (nullable = true)\n",
      " |-- water_temperature: double (nullable = true)\n",
      " |-- wind_gust_max_10min: double (nullable = true)\n",
      " |-- wind_speed_avg_10min: double (nullable = true)\n",
      " |-- wind_force_avg_10min: double (nullable = true)\n",
      " |-- wind_direction: integer (nullable = true)\n",
      " |-- windchill: double (nullable = true)\n",
      " |-- barometric_pressure_qfe: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- dew_point: double (nullable = true)\n",
      " |-- global_radiation: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- water_level: double (nullable = true)\n",
      " |-- Hour: double (nullable = true)\n",
      " |-- AccidentSeverityCategory: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- WeekDay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the weather and the accident df\n",
    "\n",
    "df_weather_temp = sql_context.read\\\n",
    "    .options(header=True, inferSchema=True)\\\n",
    "    .csv('datasets/hourly_weather.csv')\n",
    "\n",
    "df_weather = df_weather_temp.withColumn('Hour', df_weather_temp.date[12:13][1:2].cast('double'))\n",
    "\n",
    "df_accidents_temp = sql_context.read\\\n",
    "    .options(header=True, inferSchema=True)\\\n",
    "    .csv('datasets/accidents.csv')\n",
    "df_accidents = df_accidents_temp.drop('Hour')\n",
    "\n",
    "# join the two datasets\n",
    "df = df_weather.join(df_accidents, how='left')\n",
    "\n",
    "# drop all not essential columns that will not be used as features or labels\n",
    "df = df.drop(*['AccidentLocation_CHLV95_N', 'AccidentLocation_CHLV95_E', 'RoadType',\\\n",
    "               'AccidentInvolvingMotorcycle', 'AccidentInvolvingBicycle', 'AccidentInvolvingPedestrian',\\\n",
    "              'AccidentType', 'date', '_c0'])\n",
    "\n",
    "# show the schema of the joined df\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_accidents.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_weather.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark pipeline creation\n",
    "- Create spark pipeline to:\n",
    "    - Apply one hot encoding\n",
    "    - Define X matrix and y label\n",
    "    - Add all stages to the pipeline\n",
    "    \n",
    "The performance on the String indexer is very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "\n",
    "X = df.columns\n",
    "#del X[-4:]\n",
    "\n",
    "# index accident type, accident severity and week day\n",
    "\n",
    "severity_indexer = StringIndexer(inputCol='AccidentSeverityCategory', outputCol='severityIndex')\n",
    "# severity_indexer = StringIndexer(inputCol='AccidentSeverityCategory', outputCol='severityIndex')\n",
    "day_indexer = StringIndexer(inputCol='WeekDay', outputCol='weekDayIndex')\n",
    "\n",
    "stages += [severity_indexer]\n",
    "stages += [day_indexer]\n",
    "\n",
    "# One-hot encoder for accident type, severity and week day\n",
    "\n",
    "# ohe_accident_type = OneHotEncoder(inputCol='typeIndex', outputCol='type_vec')\n",
    "# ohe_accident_severity = OneHotEncoder(intype_indexerputCol='severityIndex', outputCol='severity_vec')\n",
    "ohe_weekday = OneHotEncoder(inputCol='weekDayIndex', outputCol='weekday_vec')\n",
    "ohe_hour = OneHotEncoder(inputCol='Hour', outputCol='hour_vec')\n",
    "5000\n",
    "X += ['weekday_vec', 'hour_vec']\n",
    "y = 'severityIndex'\n",
    "\n",
    "# stages += ohe_accident_type\n",
    "\n",
    "\n",
    "assembler_final = VectorAssembler(inputCols=X, outputCol='features')\n",
    "\n",
    "stages += [ohe_weekday, ohe_hour, assembler_final]\n",
    "\n",
    "# Create pipeline and pass all stages\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_5f17e8875dc4,\n",
       " StringIndexer_86799480d0e4,\n",
       " OneHotEncoder_0c46231a04fe,\n",
       " OneHotEncoder_3d7ee2486ba4,\n",
       " VectorAssembler_27b746a699d5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit pipeline to data\n",
    "pipeline_model = pipeline.fit(df)\n",
    "\n",
    "# Transform data using fitted pipeline\n",
    "df_transform = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect transformed data\n",
    "df_transform.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features and the label for the final df\n",
    "df_transform_fin = df_transform.select('features', 'severityIndex')\n",
    "df_transform_fin.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "df_transform_fin = df_transform_fin.orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Split\n",
    "train_data, test_data = df_transform_fin.randomSplit([.8, .2], seed=42)\n",
    "train_data.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "# too computationally expensive:\n",
    "# nb_classes = train_data.select('severityIndex').distinct().count()\n",
    "nb_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.select('features').limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.select('AccidentSeverityCategory').limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dimention\n",
    "#input_dim = len(train_data.select('features').first()[0])\n",
    "input_dim = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(input_dim,), activity_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "\n",
    "model.add(Dense(256, input_shape=(input_dim,), activity_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set and Serial optimizer\n",
    "optimizer_conf = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(optimizer_conf)\n",
    "\n",
    "# Initialize SparkML Estimator and get Settings\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol('features')\n",
    "estimator.setLabelCol('severityIndex')\n",
    "estimator.set_keras_model_config(model.to_yaml())\n",
    "estimator.set_categorical_labels(True)\n",
    "estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(1)\n",
    "estimator.set_epochs(2)\n",
    "estimator.set_batch_size(64)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.1)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode('synchronous')\n",
    "estimator.set_loss('categorical_crossentropy')\n",
    "estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deep learning pipeline\n",
    "dl_pipeline = Pipeline(stages=[estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for fitting, transforming and predicting train and test\n",
    "def dl_pip_fit_score_res(dl_pipeline=dl_pipeline, train_data=train_data,\n",
    "                        test_data=test_data, label='labelIndex'):\n",
    "    fit_dl_pipeline = dl_pipeline.fit(train_data)\n",
    "    pred_train = fit_dl_pipeline.transform(train_data)\n",
    "    pred_test = fit_dl_pipeline.transform(test_data)\n",
    "    \n",
    "    pnl_train = pred_train.select(label, 'prediction')\n",
    "    pnl_test = pred_test.select(label, 'prediction')\n",
    "    \n",
    "    pred_and_label_train = pnl_train.rdd.map(lambda row: (row[label], row['prediction']))\n",
    "    pred_and_label_test = pnl_test.rdd.map(lambda row: (row[label], row['prediction']))\n",
    "    \n",
    "    metrics_train = MulticlassMetrics(pred_and_label_train)\n",
    "    metrics_test = MulticlassMetrics(pred_and_label_test)\n",
    "    \n",
    "    print('Train data accuracy: {}'.format(round(metrics_train.precision(), 4)))\n",
    "    print('Training Data Confusion Matrix')\n",
    "    display(pnl_train.crosstab('labelIndex', 'prediction\\n').toPandas())\n",
    "    \n",
    "    print('Test data accuracy: {}'.format(round(metrics_test.precision(), 4)))\n",
    "    print('Test Data Confusion Matrix')\n",
    "    display(pnl_test.crosstab('labelIndex', 'prediction').toPandas())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_pip_fit_score_res(dl_pipeline=dl_pipeline, train_data=train_data,\n",
    "                        test_data=test_data, label='labelIndex')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_gpu]",
   "language": "python",
   "name": "conda-env-tensorflow_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
