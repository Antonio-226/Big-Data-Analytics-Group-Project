{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0ba173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.context import SparkContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import optimizers, regularizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f987f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('Spark DL Tabular Pipeline').setMaster('local[6]')\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "#spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11cd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the weather and the accident df\n",
    "\n",
    "df_weather = sql_context.read\\\n",
    "    .options(header=True, inferSchema=True)\\\n",
    "    .csv('datasets/hourly_weather.csv')\n",
    "df_accidents = sql_context.read\\\n",
    "    .options(header=True, inferSchema=True)\\\n",
    "    .csv('datasets/accidents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767e1882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------------+-----------------+-------------------+--------------------+--------------------+--------------+------------------+-----------------------+-------------+------------------+----------------+-----------------+-----------+---+-------------------+------------+------------------------+---------------------------+------------------------+---------------------------+--------+-------------------------+-------------------------+-----+--------+----+\n",
      "|_c0|               date|  air_temperature|water_temperature|wind_gust_max_10min|wind_speed_avg_10min|wind_force_avg_10min|wind_direction|         windchill|barometric_pressure_qfe|precipitation|         dew_point|global_radiation|         humidity|water_level|_c0|               date|AccidentType|AccidentSeverityCategory|AccidentInvolvingPedestrian|AccidentInvolvingBicycle|AccidentInvolvingMotorcycle|RoadType|AccidentLocation_CHLV95_E|AccidentLocation_CHLV95_N|Month| WeekDay|Hour|\n",
      "+---+-------------------+-----------------+-----------------+-------------------+--------------------+--------------------+--------------+------------------+-----------------------+-------------+------------------+----------------+-----------------+-----------+---+-------------------+------------+------------------------+---------------------------+------------------------+---------------------------+--------+-------------------------+-------------------------+-----+--------+----+\n",
      "|  0|2011-01-01 00:30:00|2.233333333333334|              5.2|                2.4|  1.2166666666666668|  1.2166666666666668|          1785|2.2000000000000006|      974.5500000000001|          0.0|1.6166666666666665|             0.5|95.83333333333333|     67.635|  0|2011-01-01 00:30:00|         at0|                     as4|                          0|                       0|                          0|   rt433|                  2684605|                  1245194|    1|Saturday| 0.0|\n",
      "+---+-------------------+-----------------+-----------------+-------------------+--------------------+--------------------+--------------+------------------+-----------------------+-------------+------------------+----------------+-----------------+-----------+---+-------------------+------------+------------------------+---------------------------+------------------------+---------------------------+--------+-------------------------+-------------------------+-----+--------+----+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- air_temperature: double (nullable = true)\n",
      " |-- water_temperature: double (nullable = true)\n",
      " |-- wind_gust_max_10min: double (nullable = true)\n",
      " |-- wind_speed_avg_10min: double (nullable = true)\n",
      " |-- wind_force_avg_10min: double (nullable = true)\n",
      " |-- wind_direction: integer (nullable = true)\n",
      " |-- windchill: double (nullable = true)\n",
      " |-- barometric_pressure_qfe: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- dew_point: double (nullable = true)\n",
      " |-- global_radiation: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- water_level: double (nullable = true)\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- AccidentType: string (nullable = true)\n",
      " |-- AccidentSeverityCategory: string (nullable = true)\n",
      " |-- AccidentInvolvingPedestrian: integer (nullable = true)\n",
      " |-- AccidentInvolvingBicycle: integer (nullable = true)\n",
      " |-- AccidentInvolvingMotorcycle: integer (nullable = true)\n",
      " |-- RoadType: string (nullable = true)\n",
      " |-- AccidentLocation_CHLV95_E: integer (nullable = true)\n",
      " |-- AccidentLocation_CHLV95_N: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- WeekDay: string (nullable = true)\n",
      " |-- Hour: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_weather.join(df_accidents, how='left')\n",
    "df.show(1)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f501b9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- air_temperature: double (nullable = true)\n",
      " |-- water_temperature: double (nullable = true)\n",
      " |-- wind_gust_max_10min: double (nullable = true)\n",
      " |-- wind_speed_avg_10min: double (nullable = true)\n",
      " |-- wind_force_avg_10min: double (nullable = true)\n",
      " |-- wind_direction: integer (nullable = true)\n",
      " |-- windchill: double (nullable = true)\n",
      " |-- barometric_pressure_qfe: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- dew_point: double (nullable = true)\n",
      " |-- global_radiation: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- water_level: double (nullable = true)\n",
      " |-- AccidentType: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- WeekDay: string (nullable = true)\n",
      " |-- Hour: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(*['AccidentLocation_CHLV95_N', 'AccidentLocation_CHLV95_E', 'RoadType',\\\n",
    "               'AccidentInvolvingMotorcycle', 'AccidentInvolvingBicycle', 'AccidentInvolvingPedestrian',\\\n",
    "              'AccidentSeverityCategory', 'date', '_c0'])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d346efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_feature_scale(df=df, lower_skew=-2, upper_skew=2, dtypes='int32', drop_cols=['']):\n",
    "    \n",
    "    #empty sel features\n",
    "    selected_features = []\n",
    "    \n",
    "    # Select features to scale based on inputs\n",
    "    feature_list = list(df.toPandas().select_dtypes(include=[dtypes]).columns.drop(drop_cols))\n",
    "    \n",
    "    # Loop through feature list to select features based on Kurtosis / Skew\n",
    "    for feature in feature_list:\n",
    "        if df.toPandas()[feature].kurtosis() < -2 or df.toPandas()[feature].kurtosis() > 2:\n",
    "            selected_features.append(feature)\n",
    "    # Return feature list to scale\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a071ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark pipeline\n",
    "\n",
    "\n",
    "cat_features = ['WeekDay']\n",
    "\n",
    "cat_indexed_features = ['Month', 'Hour']\n",
    "\n",
    "num_features = ['air_temperature', 'water_temperature', 'wind_gust_max_10min',\\\n",
    "                   'wind_speed_avg_10min', 'wind_force_avg_10min', 'wind_direction',\\\n",
    "                   'windchill', 'barometric_pressure_qfe', 'precipitation',\\\n",
    "                   'dew_point', 'global_radiation', 'humidity', 'water_level']\n",
    "\n",
    "label = 'AccidentType'\n",
    "\n",
    "\n",
    "# Pipeline stages list\n",
    "stages = []\n",
    "\n",
    "# Loop for stringindexer and ohe for categorical variables\n",
    "for feature in cat_features:\n",
    "    \n",
    "    # Index categorical features\n",
    "    string_indexer = StringIndexer(inputCol=feature, outputCol=feature + '_index')\n",
    "    \n",
    "    # Apply one hot encoding of categorical variables\n",
    "    encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()],\n",
    "                                    outputCols=[feature + '_class_vec'])\n",
    "    \n",
    "    # Append pipeline stages\n",
    "    stages += [string_indexer, encoder]\n",
    "    \n",
    "for feature in cat_indexed_features:\n",
    "    \n",
    "    # Apply one hot encoding of categorical variables\n",
    "    encoder = OneHotEncoder(inputCol=feature, outputCol=feature + '_class_vec')\n",
    "    \n",
    "    # Append pipeline stages\n",
    "    stages += [encoder]\n",
    "\n",
    "# Index label feature\n",
    "label_str_index = StringIndexer(inputCol=label, outputCol='label_index')\n",
    "\n",
    "# Scale feature: select the features to scale using the helper funcntion and standardize\n",
    "# unscaled_features = sel_feature_scale(df=df, lower_skew=-2, upper_skew=2, dtypes='integer', drop_cols=[])\n",
    "\n",
    "# unscaled_assembler = VectorAssembler(inputCols=unscaled_features, outputCol='unscaled_features')\n",
    "# scaler = StandardScaler(inputCol='unscaled_features', outputCol='scaled_features')\n",
    "\n",
    "# stages += [unscaled_assembler, scaler]\n",
    "\n",
    "# Create list of Numeric Features that are not being scaled\n",
    "# num_unscaled_diff_list = list(set(num_features) - set(unscaled_features))\n",
    "num_unscaled_diff_list = list(set(num_features))\n",
    "\n",
    "# Assemble or concat the categorical features and numeric features\n",
    "assembler_inputs = [feature + '_class_vec' for feature in cat_features] + [feature + '_class_vec' for feature in cat_indexed_features] + num_unscaled_diff_list\n",
    "\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol='assembled_inputs')\n",
    "\n",
    "stages += [label_str_index, assembler]\n",
    "\n",
    "# Assemble Final training data of scaled, numeric and categorical engeneered features\n",
    "assembler_final = VectorAssembler(inputCols=['scaled_features', 'assembled_inputs'], outputCol='features')\n",
    "\n",
    "stages += [assembler_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd5bde4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_899179010f3f,\n",
       " OneHotEncoder_4d38b474f2b6,\n",
       " OneHotEncoder_8f951ed0aea2,\n",
       " OneHotEncoder_94a870d32884,\n",
       " StringIndexer_b0df2f1672f3,\n",
       " VectorAssembler_baf4ff7ea461,\n",
       " VectorAssembler_b1b16cf94178]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ce631af",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o49.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 18) (192.168.1.26 executor driver): java.lang.NullPointerException: Value at index 0 is null\n\tat org.apache.spark.sql.Row.getAnyValAs(Row.scala:523)\n\tat org.apache.spark.sql.Row.getDouble(Row.scala:270)\n\tat org.apache.spark.sql.Row.getDouble$(Row.scala:270)\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$.$anonfun$getOutputAttrGroupFromData$3(OneHotEncoder.scala:520)\n\tat scala.runtime.java8.JFunction1$mcDI$sp.apply(JFunction1$mcDI$sp.java:23)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$.$anonfun$getOutputAttrGroupFromData$2(OneHotEncoder.scala:520)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2297)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$.getOutputAttrGroupFromData(OneHotEncoder.scala:521)\n\tat org.apache.spark.ml.feature.OneHotEncoder.fit(OneHotEncoder.scala:196)\n\tat org.apache.spark.ml.feature.OneHotEncoder.fit(OneHotEncoder.scala:128)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException: Value at index 0 is null\n\tat org.apache.spark.sql.Row.getAnyValAs(Row.scala:523)\n\tat org.apache.spark.sql.Row.getDouble(Row.scala:270)\n\tat org.apache.spark.sql.Row.getDouble$(Row.scala:270)\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$.$anonfun$getOutputAttrGroupFromData$3(OneHotEncoder.scala:520)\n\tat scala.runtime.java8.JFunction1$mcDI$sp.apply(JFunction1$mcDI$sp.java:23)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$.$anonfun$getOutputAttrGroupFromData$2(OneHotEncoder.scala:520)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-358b374be790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fit pipeline to data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpipeline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#tranform data using fitted pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.9/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o49.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 18) (192.168.1.26 executor driver): java.lang.NullPointerException: Value at index 0 is null\n\tat org.apache.spark.sql.Row.getAnyValAs(Row.scala:523)\n\tat org.apache.spark.sql.Row.getDouble(Row.scala:270)\n\tat org.apache.spark.sql.Row.getDouble$(Row.scala:270)\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$.$anonfun$getOutputAttrGroupFromData$3(OneHotEncoder.scala:520)\n\tat scala.runtime.java8.JFunction1$mcDI$sp.apply(JFunction1$mcDI$sp.java:23)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$.$anonfun$getOutputAttrGroupFromData$2(OneHotEncoder.scala:520)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2297)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$.getOutputAttrGroupFromData(OneHotEncoder.scala:521)\n\tat org.apache.spark.ml.feature.OneHotEncoder.fit(OneHotEncoder.scala:196)\n\tat org.apache.spark.ml.feature.OneHotEncoder.fit(OneHotEncoder.scala:128)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException: Value at index 0 is null\n\tat org.apache.spark.sql.Row.getAnyValAs(Row.scala:523)\n\tat org.apache.spark.sql.Row.getDouble(Row.scala:270)\n\tat org.apache.spark.sql.Row.getDouble$(Row.scala:270)\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$.$anonfun$getOutputAttrGroupFromData$3(OneHotEncoder.scala:520)\n\tat scala.runtime.java8.JFunction1$mcDI$sp.apply(JFunction1$mcDI$sp.java:23)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.ml.feature.OneHotEncoderCommon$.$anonfun$getOutputAttrGroupFromData$2(OneHotEncoder.scala:520)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# set pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit pipeline to data\n",
    "pipeline_model = pipeline.fit(df)\n",
    "\n",
    "#tranform data using fitted pipeline\n",
    "df_transform = pipeline_model.tranform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect transformed data\n",
    "df_transfrom.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only features and label index for the final dataframe\n",
    "df_transform_fin = df_transform.select('features', 'label_index')\n",
    "df_transform_fin.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1b62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66769850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edb144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb258b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe32ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e527dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the weather and the accident df\n",
    "\n",
    "df_weather = spark.read\\\n",
    "    .options(header=True, inferSchema=True)\\\n",
    "    .csv('datasets/hourly_weather.csv')\n",
    "df_accidents = spark.read\\\n",
    "    .options(header=True, inferSchema=True)\\\n",
    "    .csv('datasets/accidents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents = df_accidents.drop('AccidentUID', 'AccidentType_de', 'AccidentType_fr', 'AccidentType_it',\\\n",
    "                                  'AccidentType_en',\\\n",
    "                                  'AccidentSeverityCategory_de', 'AccidentSeverityCategory_fr',\\\n",
    "                                  'AccidentSeverityCategory_it', 'AccidentSeverityCategory_en',\\\n",
    "                                  'RoadType_de', 'RoadType_fr', 'RoadType_it', 'RoadType_en',\\\n",
    "                                  'AccidentLocation_CHLV95_E', 'AccidentLocation_CHLV95_N', 'CantonCode',\\\n",
    "                                  'MunicipalityCode', 'AccidentYear', 'AccidentMonth', 'AccidentMonth_de',\\\n",
    "                                  'AccidentMonth_fr', 'AccidentMonth_it', 'AccidentMonth_en', 'AccidentWeekDay',\\\n",
    "                                  'AccidentWeekDay_de', 'AccidentWeekDay_fr', 'AccidentWeekDay_it',\\\n",
    "                                  'AccidentWeekDay_en', 'AccidentHour', 'AccidentHour_text', 'day', 'RoadType',\\\n",
    "                                  'AccidentInvolvingPedestrian', 'AccidentInvolvingBicycle', 'AccidentInvolvingMotorcycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd380b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a117ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index accident type, accident severity and week day\n",
    "\n",
    "type_indexer = StringIndexer(inputCol='AccidentType', outputCol='typeIndex')\n",
    "severity_indexer = StringIndexer(inputCol='AccidentSeverityCategory', outputCol='severityIndex')\n",
    "# day_indexer = StringIndexer(inputCol='WeekDay', outputCol='weekDayIndex')\n",
    "\n",
    "# One-hot encoder for accident type and severity\n",
    "\n",
    "ohe_accident_type = OneHotEncoder(inputCol='typeIndex', outputCol='type_vec')\n",
    "ohe_accident_severity = OneHotEncoder(inputCol='severityIndex', outputCol='severity_vec')\n",
    "\n",
    "# Create pipeline and pass all stages\n",
    "pipeline = Pipeline(stages=[type_indexer,\n",
    "                           severity_indexer,\n",
    "                           ohe_accident_type,\n",
    "                           ohe_accident_severity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a58c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pipeline\n",
    "\n",
    "df_accidents_transformed = pipeline.fit(df_accidents).transform(df_accidents)\n",
    "df_accidents_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b22e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f36694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_transformed = df_weather.withColumnRenamed('air_temperature', 'air_temp')\\\n",
    "    .withColumnRenamed('water_temperature', 'water_temp')\\\n",
    "    .withColumnRenamed('wind_gust_max_10min', 'wind_gust')\\\n",
    "    .withColumnRenamed('wind_speed_avg_10min', 'avg_wind_speed')\\\n",
    "    .withColumnRenamed('wind_force_avg_10min', 'avg_wind_force')\\\n",
    "    .withColumnRenamed('barometric_pressure_qfe', 'pressure_qfe')\n",
    "# df_weather_transformed.printSchema()\n",
    "\n",
    "df_weather_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39fb221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_weather_transformed.join(df_accidents_transformed, how='left')\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b037f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter('WeekDay is NULL').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d7b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b193f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accidents_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2997603",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_2.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
