{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "HOST = '127.0.0.1'  # Standard loopback interface address (localhost)\n",
    "PORT = 65432        # Port to listen on (non-privileged ports are > 1023)\n",
    "\n",
    "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "    s.bind((HOST, PORT))\n",
    "    s.listen()\n",
    "    conn, addr = s.accept()\n",
    "    with conn:\n",
    "        print('Connected by', addr)\n",
    "        while True:\n",
    "            data = conn.recv(1024)\n",
    "            if not data:\n",
    "                break\n",
    "            conn.sendall(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url = 'https://data.stadt-zuerich.ch/api/3/action/datastore_upsert'  \n",
    "#fileobj = urllib.urlopen(url)\n",
    "#print(fileobj.read())\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "response = urllib.request.urlopen(url)\n",
    "#print(response)\n",
    "    # convert response to string\n",
    "\n",
    "string = response.read().decode('utf-8')\n",
    "#print(string)\n",
    "    # convert string to json object\n",
    "\n",
    "json_obj = json.loads(string)\n",
    "print(json_obj)\n",
    "    # create a pandas df with the records from the json object\n",
    "\n",
    "live_data = pd.DataFrame.from_dict(json_obj['result']['records'])\n",
    "    \n",
    "json_obj['result']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark\n",
    "import datetime\n",
    "\n",
    "df_weather = spark.read\\\n",
    "    .options(header=True, inferSchema=True)\\\n",
    "    .csv('datasets/hourly_weather.csv')\n",
    "\n",
    "df_accidents = spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------------+-----------------+-------------------+--------------------+--------------------+--------------+------------------+-----------------------+-------------+------------------+----------------+-----------------+-----------+\n",
      "|_c0|               date|  air_temperature|water_temperature|wind_gust_max_10min|wind_speed_avg_10min|wind_force_avg_10min|wind_direction|         windchill|barometric_pressure_qfe|precipitation|         dew_point|global_radiation|         humidity|water_level|\n",
      "+---+-------------------+-----------------+-----------------+-------------------+--------------------+--------------------+--------------+------------------+-----------------------+-------------+------------------+----------------+-----------------+-----------+\n",
      "|  0|2011-01-01 00:30:00|2.233333333333334|              5.2|                2.4|  1.2166666666666668|  1.2166666666666668|          1785|2.2000000000000006|      974.5500000000001|          0.0|1.6166666666666665|             0.5|95.83333333333333|     67.635|\n",
      "+---+-------------------+-----------------+-----------------+-------------------+--------------------+--------------------+--------------+------------------+-----------------------+-------------+------------------+----------------+-----------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('date', 'string'),\n",
       " ('air_temperature', 'double'),\n",
       " ('water_temperature', 'double'),\n",
       " ('wind_gust_max_10min', 'double'),\n",
       " ('wind_speed_avg_10min', 'double'),\n",
       " ('wind_force_avg_10min', 'double'),\n",
       " ('wind_direction', 'int'),\n",
       " ('windchill', 'double'),\n",
       " ('barometric_pressure_qfe', 'double'),\n",
       " ('precipitation', 'double'),\n",
       " ('dew_point', 'double'),\n",
       " ('global_radiation', 'double'),\n",
       " ('humidity', 'double'),\n",
       " ('water_level', 'double')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+------------------+-------------------+--------------------+--------------------+------------------+------------------+-----------------------+--------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|               _c0|               date|   air_temperature| water_temperature|wind_gust_max_10min|wind_speed_avg_10min|wind_force_avg_10min|    wind_direction|         windchill|barometric_pressure_qfe|       precipitation|         dew_point|  global_radiation|          humidity|      water_level|\n",
      "+-------+------------------+-------------------+------------------+------------------+-------------------+--------------------+--------------------+------------------+------------------+-----------------------+--------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|             78707|              78707|             78707|             78707|              78707|               78707|               78707|             78707|             78707|                  78707|               78707|             78707|             78707|             78707|            78707|\n",
      "|   mean|           39353.0|               null|   11.559715908369|13.427116584293572|  4.363192600403992|  1.8799886710627294|  1.8395339042270875| 903.5109583645673|10.406976232524809|      969.6917085943273|0.019387623294835656| 7.126690785656447|134.39075516366705| 76.59401789760302| 81.3195543090171|\n",
      "| stddev|22720.898155662773|               null| 8.010165558990991| 6.821680543032483| 2.9872225309578524|  1.2781437782435408|  1.2294307550886867|464.65170186022476| 8.608585130907478|       7.54671994033245| 0.11152811171294262|6.3497148323675425|221.16304511626367|15.994959006655913|4.182609728719894|\n",
      "|    min|                 0|2011-01-01 00:30:00|            -13.16|              2.56|                0.0|                 0.0|                 0.0|                 0|            -22.44|                 930.82|                 0.0|            -16.64|               0.0|              16.8|67.55499999999999|\n",
      "|    max|             78706|2019-12-31 22:30:00|37.120000000000005|27.919999999999998|               32.0|  14.280000000000001|  14.280000000000001|              1923|             37.36|                 994.46|                 6.4|23.300000000000004|            1079.0|             100.0|           406.07|\n",
      "+-------+------------------+-------------------+------------------+------------------+-------------------+--------------------+--------------------+------------------+------------------+-----------------------+--------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('datasets/hourly_weather.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'date',\n",
       " 'air_temperature',\n",
       " 'water_temperature',\n",
       " 'wind_gust_max_10min',\n",
       " 'wind_speed_avg_10min',\n",
       " 'wind_force_avg_10min',\n",
       " 'wind_direction',\n",
       " 'windchill',\n",
       " 'barometric_pressure_qfe',\n",
       " 'precipitation',\n",
       " 'dew_point',\n",
       " 'global_radiation',\n",
       " 'humidity',\n",
       " 'water_level']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|               date|count|\n",
      "+-------------------+-----+\n",
      "|2011-01-01 04:30:00|    1|\n",
      "|2011-01-02 00:30:00|    1|\n",
      "|2011-02-08 07:30:00|    1|\n",
      "|2011-02-13 14:30:00|    1|\n",
      "|2011-02-18 20:30:00|    1|\n",
      "|2011-02-26 15:30:00|    1|\n",
      "|2011-03-04 06:30:00|    1|\n",
      "|2011-03-14 03:30:00|    1|\n",
      "|2011-03-19 04:30:00|    1|\n",
      "|2011-03-24 02:30:00|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('date').count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "steps = df.select('date').distinct().collect()\n",
    "for step in steps[:]:\n",
    "    _df = df\n",
    "    _df.coalesce(1).write.mode(\"append\").option(\"header\", \"true\").csv(\"logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd logs/ && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = spark.read.csv(\n",
    "    'logs/part-00000-002e8f2f-8449-4795-ad3c-f97a929cc814-c000.csv',\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part.groupBy(\"date\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = part.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = (\n",
    "    spark.readStream.schema(dataSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(\"logs/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_count = streaming\n",
    "dest_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery = {\n",
    "    dest_count.writeStream.queryName(\"dest_counts\")\n",
    "    .format(\"memory\")\n",
    "    .outputMode('complete')\n",
    "    .start()\n",
    "}\n",
    "\n",
    "import time\n",
    "\n",
    "for x in range(50):\n",
    "    _df = spark.sql(\n",
    "        'SELECT * FROM dest_count'\n",
    "    )\n",
    "    if _df.count() > 1:\n",
    "        _df.show(10)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = sc.textFile('http://localhost:9870/explorer.html#/weather/hourly_weather.csv', 9870)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:9870/explorer.html#/weather/hourly_weather.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import threading\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://data.stadt-zuerich.ch/api/3/action/datastore_search?resource_id=ab29e6fa-1e79-441a-b096-37c23cf3d2be&limit=5'\n",
    "\n",
    "def getLiveData(url):\n",
    "\n",
    "    response = urllib.request.urlopen(url)\n",
    "\n",
    "    # convert response to string\n",
    "\n",
    "    string = response.read().decode('utf-8')\n",
    "\n",
    "    # convert string to json object\n",
    "\n",
    "    json_obj = json.loads(string)\n",
    "\n",
    "    # create a pandas df with the records from the json object\n",
    "\n",
    "    live_data = pd.DataFrame.from_dict(json_obj['result']['records'])\n",
    "    return live_data\n",
    "\n",
    "def writeLogs():\n",
    "    \n",
    "    # repeat function every 10 seconds\n",
    "    \n",
    "    threading.Timer(10, writeLogs).start()\n",
    "    \n",
    "    # pull data and write log file\n",
    "    \n",
    "    now = datetime.now()\n",
    "    getLiveData(data_url).to_csv('logs/log' + str(now.strftime(\"%m%d%Y;%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "# getLiveData(data_url).to_csv('logs/log' + str(now.strftime(\"%m%d%Y;%H:%M:%S\")))\n",
    "print(getLiveData(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "str(now.strftime(\"%m%d%Y;%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('logs/log05202021;14:03:16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeLogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "stream = sc.textFileStream('logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "ssc = StreamingContext(sc, 5)\n",
    "data = ssc.textFile(\"/home/bigdata/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "response = urllib.request.urlopen(data_url)\n",
    "\n",
    "# convert response to string\n",
    "\n",
    "string = response.read().decode('utf-8')\n",
    "\n",
    "# convert string to json object\n",
    "\n",
    "json_obj = json.loads(string)\n",
    "\n",
    "# create a pandas df with the records from the json object\n",
    "\n",
    "live_data = pd.DataFrame.from_dict(json_obj['result']['records'])\n",
    "live_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
