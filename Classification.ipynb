{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f12ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ba269",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e952b75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794102a",
   "metadata": {},
   "source": [
    "**Class Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Fig.1) Accident Distribution', fontweight = 'bold')\n",
    "sns.countplot(x = 'Accident', data=df, facecolor=(0, 0, 0, 0),\n",
    "                   linewidth=5, edgecolor=sns.color_palette(\"dark\", 3))\n",
    "plt.ylabel('Count', fontweight = 'bold')\n",
    "plt.xlabel('Accident', fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aeadbf",
   "metadata": {},
   "source": [
    "## I) Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2846a",
   "metadata": {},
   "source": [
    "**Create new dataset containing the relevant variables:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ff74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.columns[~df.columns.isin([col for col in df.columns if col.startswith('Accident') and col != 'Accident'])]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb78fa",
   "metadata": {},
   "source": [
    "### a) Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a0af5",
   "metadata": {},
   "source": [
    "**Create Input and Output Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[~df.columns.isin(['date', 'Accident', 'Unnamed: 0'])]]\n",
    "y = df['Accident']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444bb74",
   "metadata": {},
   "source": [
    "**Create train and test datasets:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9953e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0c662",
   "metadata": {},
   "source": [
    "**Model Parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a76194",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b83e1",
   "metadata": {},
   "source": [
    "**Define Custom Dataloaders:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd161cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data\n",
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = trainData(torch.FloatTensor(X_train), \n",
    "                       torch.FloatTensor(y_train))\n",
    "## test data    \n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = testData(torch.FloatTensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22702a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8116d5",
   "metadata": {},
   "source": [
    "**Define Neural Net Architecture**\n",
    "<img src=\"https://miro.medium.com/max/1400/0*CLjAAd7s6o0yfEYZ.jpg\"\n",
    "     alt=\"NN\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        # Number of input features is 53.\n",
    "        self.layer_1 = nn.Linear(53, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afebb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = binaryClassification()\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98611ed8",
   "metadata": {},
   "source": [
    "**Accuracy function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0778ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e703e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f014580",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, y_batch in train_loader:\n",
    "    print(y_batch.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad83f6",
   "metadata": {},
   "source": [
    "**Train the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d302c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df507187",
   "metadata": {},
   "source": [
    "**Test the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560aa36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b129a",
   "metadata": {},
   "source": [
    "**Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8327d2",
   "metadata": {},
   "source": [
    "### b) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output label\n",
    "target = np.array(df['Accident'])\n",
    "\n",
    "#input features\n",
    "features = df[df.columns[~df.columns.isin(['date','Accident', 'Unnamed: 0'])]]\n",
    "feature_list = list(features.columns)\n",
    "features = np.array(features)\n",
    "\n",
    "#split dataset into training and testing\n",
    "train_features, test_features, train_target, test_target = train_test_split(features, \n",
    "target, test_size = 0.25, random_state = 10)\n",
    "\n",
    "\n",
    "# Instantiate model with 100 decision trees\n",
    "clf = RandomForestClassifier(n_estimators= 100)\n",
    "\n",
    "# Train the model on training data\n",
    "clf.fit(train_features, train_target)\n",
    "\n",
    "predictions = clf.predict(test_features)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(test_target, predictions))\n",
    "\n",
    "#check contributions to prediction\n",
    "feature_names = df.columns[~df.columns.isin(['date','Accident', 'Unnamed: 0'])]\n",
    "feature_imp = pd.Series(clf.feature_importances_, index=feature_names.sort_values(ascending=False))\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3581b97",
   "metadata": {},
   "source": [
    "## II) Mulit-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da5279",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "<br>\n",
    "***a)***\n",
    "<br>\n",
    "Multi-class prediction for AccidentSeverity\n",
    "<br>\n",
    "***b)***\n",
    "<br>\n",
    "Multi-class prediction for AccidentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b66d3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7b64a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'date', 'air_temperature', 'water_temperature',\n",
       "       'wind_gust_max_10min', 'wind_speed_avg_10min', 'wind_force_avg_10min',\n",
       "       'wind_direction', 'windchill', 'barometric_pressure_qfe',\n",
       "       'precipitation', 'dew_point', 'global_radiation', 'humidity',\n",
       "       'water_level', 'AccidentInvolvingPedestrian',\n",
       "       'AccidentInvolvingBicycle', 'AccidentInvolvingMotorcycle', 'Accident',\n",
       "       'AccidentType_at00', 'AccidentType_at1', 'AccidentType_at2',\n",
       "       'AccidentType_at3', 'AccidentType_at4', 'AccidentType_at5',\n",
       "       'AccidentType_at6', 'AccidentType_at7', 'AccidentType_at8',\n",
       "       'AccidentType_at9', 'AccidentSeverityCategory_as2',\n",
       "       'AccidentSeverityCategory_as3', 'AccidentSeverityCategory_as4',\n",
       "       'WeekDay_Monday', 'WeekDay_Saturday', 'WeekDay_Sunday',\n",
       "       'WeekDay_Thursday', 'WeekDay_Tuesday', 'WeekDay_Wednesday', 'Month_2.0',\n",
       "       'Month_3.0', 'Month_4.0', 'Month_5.0', 'Month_6.0', 'Month_7.0',\n",
       "       'Month_8.0', 'Month_9.0', 'Month_10.0', 'Month_11.0', 'Month_12.0',\n",
       "       'Hour_1.0', 'Hour_2.0', 'Hour_3.0', 'Hour_4.0', 'Hour_5.0', 'Hour_6.0',\n",
       "       'Hour_7.0', 'Hour_8.0', 'Hour_9.0', 'Hour_10.0', 'Hour_11.0',\n",
       "       'Hour_12.0', 'Hour_13.0', 'Hour_14.0', 'Hour_15.0', 'Hour_16.0',\n",
       "       'Hour_17.0', 'Hour_18.0', 'Hour_19.0', 'Hour_20.0', 'Hour_21.0',\n",
       "       'Hour_22.0', 'Hour_23.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda1d31",
   "metadata": {},
   "source": [
    "### - Accident Severity prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87082862",
   "metadata": {},
   "source": [
    "### i) Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa117dff",
   "metadata": {},
   "source": [
    "**Create Input and Output Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44403c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'date', 'air_temperature', 'water_temperature',\n",
       "       'wind_gust_max_10min', 'wind_speed_avg_10min', 'wind_force_avg_10min',\n",
       "       'wind_direction', 'windchill', 'barometric_pressure_qfe',\n",
       "       'precipitation', 'dew_point', 'global_radiation', 'humidity',\n",
       "       'water_level', 'AccidentInvolvingPedestrian',\n",
       "       'AccidentInvolvingBicycle', 'AccidentInvolvingMotorcycle', 'Accident',\n",
       "       'AccidentType_at00', 'AccidentType_at1', 'AccidentType_at2',\n",
       "       'AccidentType_at3', 'AccidentType_at4', 'AccidentType_at5',\n",
       "       'AccidentType_at6', 'AccidentType_at7', 'AccidentType_at8',\n",
       "       'AccidentType_at9', 'AccidentSeverityCategory_as2',\n",
       "       'AccidentSeverityCategory_as3', 'AccidentSeverityCategory_as4',\n",
       "       'WeekDay_Monday', 'WeekDay_Saturday', 'WeekDay_Sunday',\n",
       "       'WeekDay_Thursday', 'WeekDay_Tuesday', 'WeekDay_Wednesday', 'Month_2.0',\n",
       "       'Month_3.0', 'Month_4.0', 'Month_5.0', 'Month_6.0', 'Month_7.0',\n",
       "       'Month_8.0', 'Month_9.0', 'Month_10.0', 'Month_11.0', 'Month_12.0',\n",
       "       'Hour_1.0', 'Hour_2.0', 'Hour_3.0', 'Hour_4.0', 'Hour_5.0', 'Hour_6.0',\n",
       "       'Hour_7.0', 'Hour_8.0', 'Hour_9.0', 'Hour_10.0', 'Hour_11.0',\n",
       "       'Hour_12.0', 'Hour_13.0', 'Hour_14.0', 'Hour_15.0', 'Hour_16.0',\n",
       "       'Hour_17.0', 'Hour_18.0', 'Hour_19.0', 'Hour_20.0', 'Hour_21.0',\n",
       "       'Hour_22.0', 'Hour_23.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e74dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['air_temperature', 'water_temperature',\n",
    "       'wind_gust_max_10min', 'wind_speed_avg_10min', 'wind_force_avg_10min',\n",
    "       'wind_direction', 'windchill', 'barometric_pressure_qfe',\n",
    "       'precipitation', 'dew_point', 'global_radiation', 'humidity',\n",
    "       'water_level', 'AccidentType_at00', 'AccidentType_at1', 'AccidentType_at2',\n",
    "       'AccidentType_at3', 'AccidentType_at4', 'AccidentType_at5',\n",
    "       'AccidentType_at6', 'AccidentType_at7', 'AccidentType_at8',\n",
    "       'AccidentType_at9', 'WeekDay_Monday', 'WeekDay_Saturday', 'WeekDay_Sunday',\n",
    "       'WeekDay_Thursday', 'WeekDay_Tuesday', 'WeekDay_Wednesday', 'Month_2.0',\n",
    "       'Month_3.0', 'Month_4.0', 'Month_5.0', 'Month_6.0', 'Month_7.0',\n",
    "       'Month_8.0', 'Month_9.0', 'Month_10.0', 'Month_11.0', 'Month_12.0',\n",
    "       'Hour_1.0', 'Hour_2.0', 'Hour_3.0', 'Hour_4.0', 'Hour_5.0', 'Hour_6.0',\n",
    "       'Hour_7.0', 'Hour_8.0', 'Hour_9.0', 'Hour_10.0', 'Hour_11.0',\n",
    "       'Hour_12.0', 'Hour_13.0', 'Hour_14.0', 'Hour_15.0', 'Hour_16.0',\n",
    "       'Hour_17.0', 'Hour_18.0', 'Hour_19.0', 'Hour_20.0', 'Hour_21.0',\n",
    "       'Hour_22.0', 'Hour_23.0']\n",
    "\n",
    "#create data frame for prediction\n",
    "df_nn_acc_type = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106503ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9cb056",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = list(df_nn_acc_type.columns[[col.startswith('Accident') for col in df_nn_acc_type.columns]])\n",
    "\n",
    "X = df_nn_acc_type[df_nn_acc_type.columns[~df_nn_acc_type.columns.isin(pred_cols)]]\n",
    "y = df_nn_acc_type[pred_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93278936",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78208a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b8ed0",
   "metadata": {},
   "source": [
    "X_train, X_test = X_train.astype(float), X_test.astype(float)\n",
    "y_train, y_test = y_train.astype(float), y_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d416f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data\n",
    "class BinaryDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        features = self.X_data[index,:]\n",
    "        labels = self.y_data[index,:]\n",
    "        \n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "        # there are 10 classes and each class can have a binary value ...\n",
    "        # ... either 0 or 1\n",
    "        label1 = torch.tensor(labels[0], dtype=torch.float32)\n",
    "        label2 = torch.tensor(labels[1], dtype=torch.float32)\n",
    "        label3 = torch.tensor(labels[2], dtype=torch.float32)\n",
    "        label4 = torch.tensor(labels[3], dtype=torch.float32)\n",
    "        label5 = torch.tensor(labels[4], dtype=torch.float32)\n",
    "        label6 = torch.tensor(labels[5], dtype=torch.float32)\n",
    "        label7 = torch.tensor(labels[6], dtype=torch.float32)\n",
    "        label8 = torch.tensor(labels[7], dtype=torch.float32)\n",
    "        label9 = torch.tensor(labels[8], dtype=torch.float32)\n",
    "        label10 = torch.tensor(labels[9], dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            'features': features,\n",
    "            'label1': label1,\n",
    "            'label2': label2,\n",
    "            'label3': label3,\n",
    "            'label4': label4,\n",
    "            'label5': label5,\n",
    "            'label6': label6,\n",
    "            'label7': label7,\n",
    "            'label8': label8,\n",
    "            'label9': label9,\n",
    "            'label10': label10,\n",
    "        \n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af355649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadBinaryModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadBinaryModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(53, 32) # 12 is the number of features\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 10)\n",
    "        \n",
    "        # we will treat each head as a binary classifier ...\n",
    "        # ... so the output features will be 1\n",
    "        self.out1 = nn.Linear(10, 1)\n",
    "        self.out2 = nn.Linear(10, 1)\n",
    "        self.out3 = nn.Linear(10, 1)\n",
    "        self.out4 = nn.Linear(10, 1)\n",
    "        self.out5 = nn.Linear(10, 1)\n",
    "        self.out6 = nn.Linear(10, 1)\n",
    "        self.out7 = nn.Linear(10, 1)\n",
    "        self.out8 = nn.Linear(10, 1)\n",
    "        self.out9 = nn.Linear(10, 1)\n",
    "        self.out10 = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        # each binary classifier head will have its own output\n",
    "        out1 = torch.sigmoid(self.out1(x))\n",
    "        out2 = torch.sigmoid(self.out2(x))\n",
    "        out3 = torch.sigmoid(self.out3(x))\n",
    "        out4 = torch.sigmoid(self.out4(x))\n",
    "        out5 = torch.sigmoid(self.out5(x))\n",
    "        out6 = torch.sigmoid(self.out6(x))\n",
    "        out7 = torch.sigmoid(self.out7(x))\n",
    "        out8 = torch.sigmoid(self.out8(x))\n",
    "        out9 = torch.sigmoid(self.out9(x))\n",
    "        out10 = torch.sigmoid(self.out10(x))\n",
    "        \n",
    "        return out1, out2, out3, out4, out5, out6, out7, out8, out9, out10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cd5031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_loss_fn(outputs, targets):\n",
    "    o1, o2, o3, o4, o5, o6, o7, o8, o9, o10 = outputs\n",
    "    t1, t2, t3, t4, t5, t6, t7, t8, t9, t10 = targets\n",
    "    l1 = nn.BCELoss()(o1, t1)\n",
    "    l2 = nn.BCELoss()(o2, t2)\n",
    "    l3 = nn.BCELoss()(o3, t3)\n",
    "    l4 = nn.BCELoss()(o4, t4)\n",
    "    l5 = nn.BCELoss()(o5, t5)\n",
    "    l6 = nn.BCELoss()(o6, t6)\n",
    "    l7 = nn.BCELoss()(o7, t7)\n",
    "    l8 = nn.BCELoss()(o8, t8)\n",
    "    l9 = nn.BCELoss()(o9, t9)\n",
    "    l10 = nn.BCELoss()(o10, t10)\n",
    "    return (l1 + l2 + l3 + l4 + l5 + l6 + l7 + l8 + l9 + l10) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c974eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BinaryDataset(X_train, y_train)\n",
    "# train data loader\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=64)\n",
    "model = MultiHeadBinaryModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "579d0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train(model, dataloader, optimizer, loss_fn, train_dataset, device):\n",
    "    model.train()\n",
    "    counter = 0\n",
    "    train_running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_dataset)/dataloader.batch_size)):\n",
    "        counter += 1\n",
    "        \n",
    "        # extract the features and labels\n",
    "        features = data['features'].to(device)\n",
    "        target1 = data['label1'].to(device)\n",
    "        target2 = data['label2'].to(device)\n",
    "        target3 = data['label3'].to(device)\n",
    "        target4 = data['label4'].to(device)\n",
    "        target5 = data['label5'].to(device)\n",
    "        target6 = data['label6'].to(device)\n",
    "        target7 = data['label7'].to(device)\n",
    "        target8 = data['label8'].to(device)\n",
    "        target9 = data['label9'].to(device)\n",
    "        target10 = data['label10'].to(device)\n",
    "        \n",
    "        # zero-out the optimizer gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(features)\n",
    "        targets = (target1, target2, target3, target4, target5, target6, target7, target8, target9, target10)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update optimizer parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = train_running_loss / counter\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47e152c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadBinaryModel(\n",
       "  (fc1): Linear(in_features=53, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=10, bias=True)\n",
       "  (out1): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (out2): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (out3): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (out4): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (out5): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (out6): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (out7): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (out8): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (out9): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (out10): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 100\n",
    "# load the model on to the computation device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6f2654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = binary_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "644c376c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/963 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0c3d5e707871>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch+1} of {epochs}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     train_epoch_loss = train(\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_loss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     )\n",
      "\u001b[1;32m<ipython-input-21-66c6848cfa08>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, loss_fn, train_dataset, device)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mtrain_running_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-909790602eb2>\u001b[0m in \u001b[0;36mbinary_loss_fn\u001b[1;34m(outputs, targets)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mo1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo10\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt10\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0ml1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0ml2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0ml3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2751\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2753\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   2754\u001b[0m             \u001b[1;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2755\u001b[0m             \u001b[1;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(\n",
    "        model, train_dataloader, optimizer, binary_loss_fn, train_dataset, device\n",
    "    )\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "torch.save(model.state_dict(), 'outputs/multi_head_binary.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e94cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937bbe13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8a948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiHeadBinaryModel()\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed70408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(multiClassification, self).__init__()\n",
    "        # Number of input features is 53.\n",
    "        self.layer_1 = nn.Linear(53, 64)\n",
    "        self.layer_2 = nn.Linear(64, 32)\n",
    "        self.layer_out = nn.Linear(32, 10)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_out(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(y_pred)\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum()\n",
    "    acc = correct_results_sum/(y_test.shape[0]*y_test.shape[1]*100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd44b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        acc = multi_acc(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16684183",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6416abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_acc(torch.Tensor(np.array(y_pred_list)), torch.Tensor(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2f19c9",
   "metadata": {},
   "source": [
    "### ii) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of columns that are not necessary for prediction\n",
    "cols = ['Unnamed: 0', 'date', 'AccidentInvolvingPedestrian',\n",
    "       'AccidentInvolvingBicycle', 'AccidentInvolvingMotorcycle', 'Accident',\n",
    "        'AccidentType_at00', 'AccidentType_at1', 'AccidentType_at2',\n",
    "       'AccidentType_at3', 'AccidentType_at4', 'AccidentType_at5',\n",
    "       'AccidentType_at6', 'AccidentType_at7', 'AccidentType_at8',\n",
    "       'AccidentType_at9']\n",
    "\n",
    "\n",
    "#create data frame for Random Forest prediction\n",
    "df_rf_acc_type = df[df.columns[~df.columns.isin(cols)]]\n",
    "df_rf_acc_type.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e5f1dc",
   "metadata": {},
   "source": [
    "**Define targets and labels and run the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6df258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#output label\n",
    "output_class = df_rf_acc_type[[col for col in df_rf_acc_type.columns if col.startswith('Accident')]]\n",
    "target = np.array(output_class)\n",
    "\n",
    "#input features\n",
    "features = df_rf_acc_type[df_rf_acc_type.columns[~df_rf_acc_type.columns.isin(output_class)]]\n",
    "feature_list = list(features.columns)\n",
    "features = np.array(features)\n",
    "\n",
    "#split dataset into training and testing\n",
    "train_features, test_features, train_target, test_target = train_test_split(features, \n",
    "target, test_size = 0.33, random_state = 32)\n",
    "\n",
    "\n",
    "# Instantiate model with 100 decision trees\n",
    "clf = RandomForestClassifier(n_estimators= 250)\n",
    "\n",
    "# Train the model on training data\n",
    "clf.fit(train_features, train_target)\n",
    "\n",
    "predictions = clf.predict(test_features)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(test_target, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e30e51",
   "metadata": {},
   "source": [
    "### - Accident Type prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a52a84",
   "metadata": {},
   "source": [
    "### i) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d749cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae3d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Unnamed: 0', 'date', 'AccidentInvolvingPedestrian',\n",
    "       'AccidentInvolvingBicycle', 'AccidentInvolvingMotorcycle', 'Accident',\n",
    "        'AccidentType_at00', 'AccidentType_at1', 'AccidentType_at2',\n",
    "       'AccidentType_at3', 'AccidentType_at4', 'AccidentType_at5',\n",
    "       'AccidentType_at6', 'AccidentType_at7', 'AccidentType_at8',\n",
    "       'AccidentType_at9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame for Random Forest prediction\n",
    "df_nn_acc_type = df[df.columns[~df.columns.isin(cols)]]\n",
    "\n",
    "X = df_nn_acc_type[df_nn_acc_type.columns[~df_nn_acc_type.columns.isin(cols)]]\n",
    "y = df_nn_acc_type[['AccidentSeverityCategory_as2', 'AccidentSeverityCategory_as3', 'AccidentSeverityCategory_as4']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b8080",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db990c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data\n",
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = trainData(torch.FloatTensor(X_train), \n",
    "                       torch.FloatTensor(y_train))\n",
    "## test data    \n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = testData(torch.FloatTensor(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f994572",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        # Number of input features is 53.\n",
    "        self.layer_1 = nn.Linear(53, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f0f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = binaryClassification()\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddecd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f0b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e512312",
   "metadata": {},
   "source": [
    "### ii) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of columns that are necessary for prediction\n",
    "cols = ['air_temperature', 'water_temperature',\n",
    "       'wind_gust_max_10min', 'wind_speed_avg_10min', 'wind_force_avg_10min',\n",
    "       'wind_direction', 'windchill', 'barometric_pressure_qfe',\n",
    "       'precipitation', 'dew_point', 'global_radiation', 'humidity',\n",
    "       'water_level', 'AccidentType_at00', 'AccidentType_at1', 'AccidentType_at2',\n",
    "       'AccidentType_at3', 'AccidentType_at4', 'AccidentType_at5',\n",
    "       'AccidentType_at6', 'AccidentType_at7', 'AccidentType_at8',\n",
    "       'AccidentType_at9',\n",
    "       'WeekDay_Monday', 'WeekDay_Saturday', 'WeekDay_Sunday',\n",
    "       'WeekDay_Thursday', 'WeekDay_Tuesday', 'WeekDay_Wednesday', 'Month_2.0',\n",
    "       'Month_3.0', 'Month_4.0', 'Month_5.0', 'Month_6.0', 'Month_7.0',\n",
    "       'Month_8.0', 'Month_9.0', 'Month_10.0', 'Month_11.0', 'Month_12.0',\n",
    "       'Hour_1.0', 'Hour_2.0', 'Hour_3.0', 'Hour_4.0', 'Hour_5.0', 'Hour_6.0',\n",
    "       'Hour_7.0', 'Hour_8.0', 'Hour_9.0', 'Hour_10.0', 'Hour_11.0',\n",
    "       'Hour_12.0', 'Hour_13.0', 'Hour_14.0', 'Hour_15.0', 'Hour_16.0',\n",
    "       'Hour_17.0', 'Hour_18.0', 'Hour_19.0', 'Hour_20.0', 'Hour_21.0',\n",
    "       'Hour_22.0', 'Hour_23.0']\n",
    "\n",
    "#Create data frame for Random Forest prediction\n",
    "df_rf_acc_sev = df[df.columns[df.columns.isin(cols)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c4351",
   "metadata": {},
   "source": [
    "**Define targets and labels and run the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173daa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output label\n",
    "output_class = df_rf_acc_sev[[col for col in df_rf_acc_sev.columns if col.startswith('Accident')]]\n",
    "target = np.array(output_class)\n",
    "\n",
    "#input features\n",
    "features = df_rf_acc_sev[df_rf_acc_sev.columns[~df_rf_acc_sev.columns.isin(output_class)]]\n",
    "feature_list = list(features.columns)\n",
    "features = np.array(features)\n",
    "\n",
    "#split dataset into training and testing\n",
    "train_features, test_features, train_target, test_target = train_test_split(features, \n",
    "target, test_size = 0.25, random_state = 11)\n",
    "\n",
    "\n",
    "# Instantiate model with 100 decision trees\n",
    "clf = RandomForestClassifier(n_estimators= 130)\n",
    "\n",
    "# Train the model on training data\n",
    "clf.fit(train_features, train_target)\n",
    "\n",
    "predictions = clf.predict(test_features)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(test_target, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
